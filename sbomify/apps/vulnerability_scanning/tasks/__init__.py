"""Vulnerability scanning tasks using the unified service layer."""

import logging
import os
from contextlib import contextmanager
from datetime import timedelta
from typing import Any, Dict

import dramatiq
from django.core.cache import cache
from django.db import transaction
from tenacity import (
    before_sleep_log,
    retry,
    retry_if_exception_type,
    stop_after_delay,
    wait_exponential,
)

# Set up Django
os.environ.setdefault("DJANGO_SETTINGS_MODULE", "sbomify.settings")
import django  # noqa: E402

django.setup()

from django.db import connection  # noqa: E402
from django.db.utils import DatabaseError, OperationalError  # noqa: E402
from django.utils import timezone  # noqa: E402

# Note: Broker configuration is handled in sbomify.tasks module
from sbomify.apps.core.models import Release  # noqa: E402
from sbomify.apps.sboms.models import SBOM  # noqa: E402
from sbomify.apps.sboms.utils import SBOMDataError, get_sbom_data_bytes  # noqa: E402
from sbomify.task_utils import format_task_error, record_task_breadcrumb, sbom_processing_task  # noqa: E402

from ..clients import VulnerabilityProviderError  # noqa: E402
from ..models import VulnerabilityScanResult  # noqa: E402
from ..services import VulnerabilityScanningService  # noqa: E402

logger = logging.getLogger(__name__)

_SCAN_LOCK_TTL_SECONDS = 60 * 30  # 30 minutes


@contextmanager
def _acquire_scan_lock(sbom_id: str):
    """
    Context manager for acquiring and releasing scan locks.

    Ensures the lock is always released, even if an exception occurs or early return happens.

    Args:
        sbom_id: SBOM ID to create lock for

    Yields:
        bool: True if lock was acquired, False if lock already exists
    """
    lock_key = f"task:sbom:vuln_scan:{sbom_id}"
    lock_acquired = cache.add(lock_key, "1", timeout=_SCAN_LOCK_TTL_SECONDS)
    try:
        yield lock_acquired
    finally:
        if lock_acquired:
            cache.delete(lock_key)


# Import cron decorator for scheduling (requires dramatiq-crontab)
# pip install dramatiq-crontab
try:
    from dramatiq_crontab import cron

    logger.info("dramatiq-crontab available - cron scheduling enabled")
except ImportError:
    logger.warning("dramatiq-crontab not installed - cron scheduling disabled, tasks will need manual triggering")

    # Fallback for environments without dramatiq-crontab
    def cron(schedule):
        """Fallback decorator when dramatiq-crontab is not installed."""

        def decorator(func):
            return func

        return decorator


def calculate_scan_timeout(sbom_data_bytes: bytes) -> int:
    """
    Calculate dynamic timeout based on SBOM size.

    Args:
        sbom_data_bytes: SBOM content as bytes

    Returns:
        Timeout in milliseconds
    """
    # Base timeout: 5 minutes
    base_timeout_ms = 300000

    # Additional time based on size
    size_mb = len(sbom_data_bytes) / (1024 * 1024)

    if size_mb < 1:
        return base_timeout_ms  # 5 minutes for small SBOMs
    elif size_mb < 10:
        return base_timeout_ms * 2  # 10 minutes for medium SBOMs
    elif size_mb < 50:
        return base_timeout_ms * 3  # 15 minutes for large SBOMs
    else:
        return base_timeout_ms * 4  # 20 minutes for very large SBOMs


@sbom_processing_task(queue_name="sbom_processing", time_limit=1200000)  # Default 20 minutes
def scan_sbom_for_vulnerabilities_unified(sbom_id: str) -> Dict[str, Any]:
    """
    Unified vulnerability scanning task that routes to OSV or Dependency Track based on team settings.

    This replaces the original OSV-only scanning task with a provider-agnostic approach.
    Features enhanced error handling and dynamic timeouts.
    """
    logger.info(f"[TASK_scan_sbom_for_vulnerabilities_unified] Starting vulnerability scan for SBOM ID: {sbom_id}")
    record_task_breadcrumb("scan_sbom_for_vulnerabilities_unified", "start", data={"sbom_id": sbom_id})

    with _acquire_scan_lock(sbom_id) as lock_acquired:
        if not lock_acquired:
            logger.info(f"[TASK_scan_sbom_for_vulnerabilities_unified] Scan already in progress for SBOM {sbom_id}")
            record_task_breadcrumb(
                "scan_sbom_for_vulnerabilities_unified",
                "skipped_duplicate",
                data={"sbom_id": sbom_id},
            )
            return {
                "status": "skipped",
                "reason": "duplicate",
                "sbom_id": sbom_id,
            }

        # Track task execution metadata
        start_time = timezone.now()
        task_metadata = {
            "sbom_id": sbom_id,
            "start_time": start_time,  # Will be converted to ISO format before return
            "provider": None,
            "sbom_size_mb": 0,
            "error_type": None,
        }

        try:
            # 1. Fetch SBOM data with timeout handling
            try:
                sbom_instance, sbom_data_bytes = get_sbom_data_bytes(sbom_id)
                task_metadata["sbom_size_mb"] = len(sbom_data_bytes) / (1024 * 1024)
                record_task_breadcrumb(
                    "scan_sbom_for_vulnerabilities_unified",
                    "sbom_loaded",
                    data={"sbom_id": sbom_id, "size_mb": task_metadata["sbom_size_mb"]},
                )

                logger.info(
                    f"[TASK_scan_sbom_for_vulnerabilities_unified] SBOM ID: {sbom_id} fetched. "
                    f"Size: {task_metadata['sbom_size_mb']:.2f}MB, "
                    f"Filename: {sbom_instance.sbom_filename}, Team: {sbom_instance.component.team.key}"
                )
            except Exception as e:
                task_metadata["error_type"] = "sbom_fetch_error"
                logger.error(f"Failed to fetch SBOM data for {sbom_id}: {str(e)}")
                raise SBOMDataError(f"Failed to fetch SBOM data: {str(e)}")

            # 2. Determine optimal timeout based on SBOM size
            calculated_timeout = calculate_scan_timeout(sbom_data_bytes)
            logger.info(
                f"Using dynamic timeout: {calculated_timeout / 1000:.1f} seconds "
                f"for SBOM size {task_metadata['sbom_size_mb']:.2f}MB"
            )

            # 3. Perform vulnerability scan with enhanced error handling
            try:
                service = VulnerabilityScanningService()
                results = service.scan_sbom_for_vulnerabilities(sbom_instance, sbom_data_bytes, scan_trigger="upload")

                task_metadata["provider"] = results.get("provider", "unknown")
                end_time = timezone.now()
                task_metadata["end_time"] = end_time.isoformat()
                task_metadata["duration_seconds"] = (end_time - start_time).total_seconds()
                # Convert start_time to ISO format for JSON serialization
                task_metadata["start_time"] = start_time.isoformat()

                logger.info(
                    f"[TASK_scan_sbom_for_vulnerabilities_unified] "
                    f"Completed vulnerability scan for SBOM ID: {sbom_id}. "
                    f"Provider: {task_metadata['provider']}, Duration: {task_metadata['duration_seconds']:.1f}s, "
                    f"Results: {results.get('summary', 'No summary available')}"
                )
                record_task_breadcrumb(
                    "scan_sbom_for_vulnerabilities_unified",
                    "completed",
                    data={"sbom_id": sbom_id, "provider": task_metadata.get("provider")},
                )

                return {
                    **results,
                    "task_metadata": task_metadata,
                }

            except VulnerabilityProviderError as e:
                task_metadata["error_type"] = "provider_error"
                logger.error(f"Vulnerability provider error for SBOM {sbom_id}: {str(e)}")
                return format_task_error("scan_sbom_for_vulnerabilities_unified", sbom_id, f"Provider error: {str(e)}")

            except TimeoutError as e:
                task_metadata["error_type"] = "timeout_error"
                logger.error(f"Vulnerability scan timeout for SBOM {sbom_id}: {str(e)}")
                return format_task_error(
                    "scan_sbom_for_vulnerabilities_unified",
                    sbom_id,
                    f"Scan timeout after {calculated_timeout / 1000:.1f}s: {str(e)}",
                )

            except Exception as e:
                task_metadata["error_type"] = "unexpected_error"
                logger.error(f"Unexpected error during vulnerability scan for SBOM {sbom_id}: {str(e)}", exc_info=True)
                return format_task_error(
                    "scan_sbom_for_vulnerabilities_unified", sbom_id, f"Unexpected error: {str(e)}"
                )

        except SBOMDataError as e:
            task_metadata["error_type"] = "sbom_data_error"
            return format_task_error("scan_sbom_for_vulnerabilities_unified", sbom_id, str(e))

        except Exception as e:
            task_metadata["error_type"] = "critical_error"
            logger.critical(f"Critical error in vulnerability scan task for SBOM {sbom_id}: {str(e)}", exc_info=True)
            return format_task_error("scan_sbom_for_vulnerabilities_unified", sbom_id, f"Critical error: {str(e)}")

        finally:
            # Log task completion metadata
            end_time = timezone.now()
            total_duration = (end_time - start_time).total_seconds()
            logger.info(
                f"[TASK_scan_sbom_for_vulnerabilities_unified] Task finished for SBOM {sbom_id}. "
                f"Total duration: {total_duration:.1f}s, Error type: {task_metadata.get('error_type', 'none')}"
            )


@cron("0 2 * * Sun")  # Run weekly on Sundays at 2 AM
@dramatiq.actor(queue_name="weekly_vulnerability_scan", max_retries=3, time_limit=7200000, store_results=True)
@retry(
    retry=retry_if_exception_type((OperationalError, DatabaseError)),
    wait=wait_exponential(multiplier=1, min=2, max=30),
    stop=stop_after_delay(300),
    before_sleep=before_sleep_log(logger, logging.WARNING),
)
def weekly_vulnerability_scan_task(
    days_back: int = 7, team_key: str = None, force_rescan: bool = False, max_releases: int = None
) -> Dict[str, Any]:
    """
    Comprehensive weekly vulnerability scanning task - automatically scheduled every Sunday at 2 AM.

    This task ensures complete vulnerability coverage by scanning:
    1. All SBOMs in recent releases (for release-based reporting)
    2. Latest SBOM for each component (for component-based reporting)

    This ensures both dashboard views (product/release focused) and component
    views (latest SBOM focused) have current vulnerability data.

    Args:
        days_back: Only scan releases created in the last N days (default: 7)
        team_key: Only scan components/releases for a specific team (for testing)
        force_rescan: Force rescan even if recent scans exist
        max_releases: Maximum number of releases to scan (for testing)

    Returns:
        Dictionary with scan statistics and results
    """
    logger.info(f"[TASK_weekly_vulnerability_scan] Starting weekly vulnerability scan at {timezone.now()}")

    try:
        # Get comprehensive scan targets (releases + latest component SBOMs)
        scan_targets = _get_comprehensive_scan_targets(days_back, team_key, max_releases)

        if not scan_targets["sboms"]:
            logger.info("[TASK_weekly_vulnerability_scan] No SBOMs found for scanning")
            return {
                "status": "completed",
                "total_releases": scan_targets["total_releases"],
                "total_components": scan_targets["total_components"],
                "total_sboms": 0,
                "successful_scans": 0,
                "failed_scans": 0,
                "skipped_scans": 0,
                "message": "No SBOMs found for scanning",
            }

        # Perform scans
        scan_results = _perform_comprehensive_scans(scan_targets, force_rescan)

        logger.info(
            f"[TASK_weekly_vulnerability_scan] Comprehensive vulnerability scan completed. "
            f"Processed {scan_results['total_releases']} releases, "
            f"{scan_results['total_components']} components, "
            f"{scan_results['successful_scans']} successful scans"
        )

        return {"status": "completed", **scan_results, "completed_at": timezone.now().isoformat()}

    except Exception:
        logger.exception("[TASK_weekly_vulnerability_scan] Weekly vulnerability scan failed")
        return {"status": "failed", "error": "Task failed", "failed_at": timezone.now().isoformat()}


def _get_comprehensive_scan_targets(days_back: int, team_key: str = None, max_releases: int = None) -> Dict[str, Any]:
    """Get comprehensive list of SBOMs to scan from both releases and latest component SBOMs."""
    from sbomify.apps.core.models import Component

    # Track all SBOMs to scan (deduplicated)
    sboms_to_scan = {}  # sbom_id -> (sbom, source_type, source_info)

    # 1. Get SBOMs from recent releases
    release_queryset = Release.objects.select_related("product__team").prefetch_related("artifacts__sbom")

    # Filter by team if specified
    if team_key:
        release_queryset = release_queryset.filter(product__team__key=team_key)

    # Filter by creation date
    if days_back:
        cutoff_date = timezone.now() - timedelta(days=days_back)
        release_queryset = release_queryset.filter(created_at__gte=cutoff_date)

    # Only get releases that have SBOMs
    release_queryset = release_queryset.filter(artifacts__sbom__isnull=False).distinct()

    # Apply max limit if specified
    if max_releases:
        release_queryset = release_queryset[:max_releases]

    releases = list(release_queryset)

    # Add SBOMs from releases
    for release in releases:
        for artifact in release.artifacts.filter(sbom__isnull=False):
            sbom = artifact.sbom
            if sbom.id not in sboms_to_scan:
                sboms_to_scan[sbom.id] = (sbom, "release", f"{release.product.name} v{release.name}")

    # 2. Get latest SBOMs from all components
    component_queryset = Component.objects.select_related("team").prefetch_related("sbom_set")

    # Filter by team if specified
    if team_key:
        component_queryset = component_queryset.filter(team__key=team_key)

    # Only get components that have SBOMs
    component_queryset = component_queryset.filter(sbom__isnull=False).distinct()

    components = list(component_queryset)

    # Add latest SBOMs from components (if not already included)
    for component in components:
        latest_sbom = component.latest_sbom
        if latest_sbom and latest_sbom.id not in sboms_to_scan:
            sboms_to_scan[latest_sbom.id] = (latest_sbom, "component_latest", f"{component.name}")

    logger.info(
        f"[TASK_weekly_vulnerability_scan] Found {len(releases)} releases and {len(components)} components. "
        f"Total unique SBOMs to scan: {len(sboms_to_scan)}"
    )

    return {
        "sboms": list(sboms_to_scan.values()),
        "total_releases": len(releases),
        "total_components": len(components),
        "total_unique_sboms": len(sboms_to_scan),
    }


def _perform_comprehensive_scans(scan_targets: Dict[str, Any], force_rescan: bool) -> Dict[str, Any]:
    """Perform vulnerability scans on all collected SBOMs."""
    sboms_list = scan_targets["sboms"]
    results = {
        "total_releases": scan_targets["total_releases"],
        "total_components": scan_targets["total_components"],
        "total_sboms": len(sboms_list),
        "successful_scans": 0,
        "failed_scans": 0,
        "skipped_scans": 0,
        "errors": [],
        "provider_stats": {},
        "source_stats": {},  # Track scans by source type (release vs component_latest)
    }

    service = VulnerabilityScanningService()

    for i, (sbom, source_type, source_info) in enumerate(sboms_list, 1):
        logger.info(
            f"[TASK_weekly_vulnerability_scan] [{i}/{len(sboms_list)}] Scanning {sbom.name} "
            f"({source_type}: {source_info})..."
        )

        team = sbom.component.team

        # Check if team has vulnerability scanning enabled
        if not _team_has_vulnerability_scanning(team):
            logger.info(f"[TASK_weekly_vulnerability_scan] Skipping {team.key} - no vulnerability scanning plan")
            results["skipped_scans"] += 1
            continue

        try:
            # Check if recent scan exists (unless force rescan)
            if not force_rescan and _has_recent_scan(sbom):
                logger.info(f"[TASK_weekly_vulnerability_scan] Skipping {sbom.name} - recent scan exists")
                results["skipped_scans"] += 1
                continue

            # Perform scan with appropriate trigger based on source
            scan_trigger = "weekly" if source_type == "release" else "component_latest"
            scan_result = _scan_sbom_comprehensive(sbom, service, scan_trigger)

            if scan_result and scan_result.get("status") != "error":
                results["successful_scans"] += 1
                provider = scan_result.get("provider", "unknown")
                results["provider_stats"][provider] = results["provider_stats"].get(provider, 0) + 1
                results["source_stats"][source_type] = results["source_stats"].get(source_type, 0) + 1
                logger.info(
                    f"[TASK_weekly_vulnerability_scan] Successfully scanned {sbom.name} with {provider} "
                    f"(source: {source_type})"
                )
            else:
                results["failed_scans"] += 1
                error_msg = scan_result.get("error", "Unknown error") if scan_result else "Scan returned None"
                results["errors"].append(f"Failed to scan {sbom.name}: {error_msg}")
                logger.error(f"[TASK_weekly_vulnerability_scan] Failed to scan {sbom.name}: {error_msg}")

        except Exception as e:
            results["failed_scans"] += 1
            error_msg = f"Failed to scan {sbom.name}: {e}"
            results["errors"].append(error_msg)
            logger.exception(f"[TASK_weekly_vulnerability_scan] {error_msg}")

    logger.info(f"[TASK_weekly_vulnerability_scan] Scan breakdown by source: {results['source_stats']}")

    return results


def _team_has_vulnerability_scanning(team) -> bool:
    """Check if team has vulnerability scanning enabled."""
    # OSV vulnerability scanning is available for ALL teams (community, business, enterprise)
    # The VulnerabilityScanningService will handle provider selection:
    # - Community teams: OSV only
    # - Business/Enterprise teams: OSV or Dependency Track based on team settings
    return True


def _has_recent_scan(sbom: SBOM) -> bool:
    """Check if SBOM has been scanned recently (within 24 hours)."""
    cutoff = timezone.now() - timedelta(hours=24)
    return VulnerabilityScanResult.objects.filter(sbom=sbom, created_at__gte=cutoff).exists()


def _scan_sbom_comprehensive(sbom: SBOM, service: VulnerabilityScanningService, scan_trigger: str) -> Dict[str, Any]:
    """Scan a single SBOM for vulnerabilities in comprehensive scanning context."""
    try:
        # Use shared utility for SBOM data fetching
        _, sbom_data = get_sbom_data_bytes(str(sbom.id))

        # Perform scan with appropriate trigger
        scan_result = service.scan_sbom_for_vulnerabilities(sbom=sbom, sbom_data=sbom_data, scan_trigger=scan_trigger)

        return {
            "status": "success",
            "provider": scan_result.get("provider", "unknown"),
            "summary": scan_result.get("summary", {}),
            "sbom_id": str(sbom.id),
            "scan_trigger": scan_trigger,
        }

    except SBOMDataError as e:
        logger.warning(f"[TASK_weekly_vulnerability_scan] {e}")
        return {"status": "error", "error": str(e)}
    except Exception:
        logger.exception(f"[TASK_weekly_vulnerability_scan] Failed to scan SBOM {sbom.id}")
        return {"status": "error", "error": "Processing error"}


@cron("0 */6 * * *")  # Run every 6 hours
@dramatiq.actor(queue_name="dt_health_check", max_retries=2, time_limit=60000, store_results=True)
@retry(
    retry=retry_if_exception_type((OperationalError, DatabaseError)),
    wait=wait_exponential(multiplier=1, min=1, max=5),
    stop=stop_after_delay(30),
    before_sleep=before_sleep_log(logger, logging.WARNING),
)
def check_dependency_track_health_task(server_id: str = None) -> Dict[str, Any]:
    """
    Task to check Dependency Track server health and update status - runs every 6 hours.

    Args:
        server_id: Specific server ID to check. If None, checks all active servers.

    Returns:
        Health check results
    """
    logger.info("[TASK_check_dependency_track_health] Starting DT health check task")

    try:
        with transaction.atomic():
            connection.ensure_connection()

            from ..models import DependencyTrackServer
            from ..services import VulnerabilityScanningService

            service = VulnerabilityScanningService()

            if server_id:
                # Check specific server
                try:
                    server = DependencyTrackServer.objects.get(id=server_id)
                    result = service.check_dependency_track_server_health(server)
                    logger.info(f"[TASK_check_dependency_track_health] Health check completed for server {server.name}")
                    return {
                        "status": "completed",
                        "servers_checked": 1,
                        "results": [result],
                        "completed_at": timezone.now().isoformat(),
                    }
                except DependencyTrackServer.DoesNotExist:
                    logger.error(f"[TASK_check_dependency_track_health] Server with ID {server_id} not found")
                    return {
                        "status": "error",
                        "error": f"Server with ID {server_id} not found",
                        "failed_at": timezone.now().isoformat(),
                    }
            else:
                # Check all active servers
                results = service.check_all_dependency_track_servers_health()
                healthy_count = sum(1 for r in results if r["status"] == "healthy")
                unhealthy_count = len(results) - healthy_count

                logger.info(
                    f"[TASK_check_dependency_track_health] Health check completed for {len(results)} servers. "
                    f"Healthy: {healthy_count}, Unhealthy: {unhealthy_count}"
                )

                return {
                    "status": "completed",
                    "servers_checked": len(results),
                    "healthy_servers": healthy_count,
                    "unhealthy_servers": unhealthy_count,
                    "results": results,
                    "completed_at": timezone.now().isoformat(),
                }

    except Exception:
        logger.exception("[TASK_check_dependency_track_health] DT health check task failed")
        return {"status": "failed", "error": "Task failed", "failed_at": timezone.now().isoformat()}


@dramatiq.actor(queue_name="dt_polling", max_retries=0, time_limit=300000, store_results=True)
@retry(
    retry=retry_if_exception_type((OperationalError, DatabaseError)),
    wait=wait_exponential(multiplier=1, min=1, max=5),
    stop=stop_after_delay(30),
    before_sleep=before_sleep_log(logger, logging.WARNING),
)
def poll_dependency_track_results_task(
    sbom_id: str, mapping_id: str, max_attempts: int = 6, initial_delay: int = 30, current_attempt: int = 1
) -> Dict[str, Any]:
    """
    Poll Dependency Track for vulnerability scan results.

    This task waits for DT to process the uploaded SBOM and then fetches
    the vulnerability results when they're ready.

    Args:
        sbom_id: SBOM ID to check results for
        mapping_id: ComponentDependencyTrackMapping ID
        max_attempts: Maximum number of polling attempts
        initial_delay: Initial delay before first check (seconds)
        current_attempt: Current attempt number

    Returns:
        Dictionary with polling results
    """
    import time

    from ..models import ComponentDependencyTrackMapping

    logger.info(f"[TASK_poll_dt_results] Attempt {current_attempt}/{max_attempts} for SBOM {sbom_id}")

    try:
        # Wait for the specified delay before checking
        if current_attempt == 1:
            delay = initial_delay
        else:
            # Exponential backoff: 30s, 60s, 120s, 240s, 480s, 960s
            delay = initial_delay * (2 ** (current_attempt - 1))
            delay = min(delay, 960)  # Cap at 16 minutes

        logger.info(f"[TASK_poll_dt_results] Waiting {delay} seconds before checking SBOM {sbom_id}")
        time.sleep(delay)

        with transaction.atomic():
            connection.ensure_connection()

            # Get SBOM and mapping
            sbom = SBOM.objects.get(id=sbom_id)
            mapping = ComponentDependencyTrackMapping.objects.get(id=mapping_id)

            # Check if DT has processed the SBOM
            service = VulnerabilityScanningService()

            try:
                # Try to get results from DT
                scan_results = service.get_dependency_track_results(sbom, mapping, force_refresh=True)

                # Check if we got meaningful results (DT has processed the SBOM)
                total_vulns = scan_results.get("vulnerability_count", {}).get("total", 0)
                scan_results.get("findings", [])

                # Consider it processed if we have vulnerability data OR if the project exists and returns zero vulns
                # (some SBOMs legitimately have no vulnerabilities)
                project_exists = scan_results.get("metrics") is not None

                if project_exists:
                    # DT has processed the SBOM, store final results
                    logger.info(
                        f"[TASK_poll_dt_results] DT processing complete for SBOM {sbom_id}. "
                        f"Found {total_vulns} vulnerabilities"
                    )

                    # Store the final results in the database
                    service._cache_dt_results(sbom, mapping, scan_results)

                    return {
                        "status": "completed",
                        "sbom_id": sbom_id,
                        "attempt": current_attempt,
                        "total_vulnerabilities": total_vulns,
                        "processing_time_seconds": delay * current_attempt,
                        "message": f"Vulnerability scan completed. Found {total_vulns} vulnerabilities.",
                        "completed_at": timezone.now().isoformat(),
                    }
                else:
                    # DT hasn't finished processing yet
                    if current_attempt >= max_attempts:
                        # Max attempts reached, give up
                        logger.warning(
                            f"[TASK_poll_dt_results] Max attempts ({max_attempts}) reached for SBOM {sbom_id}"
                        )

                        # Store a timeout result
                        timeout_results = {
                            "vulnerability_count": {
                                "critical": 0,
                                "high": 0,
                                "medium": 0,
                                "low": 0,
                                "info": 0,
                                "total": 0,
                            },
                            "findings": [],
                            "error": "Dependency Track processing timeout",
                            "scan_metadata": {"timeout": True, "attempts": max_attempts},
                        }
                        service._cache_dt_results(sbom, mapping, timeout_results)

                        return {
                            "status": "timeout",
                            "sbom_id": sbom_id,
                            "attempt": current_attempt,
                            "message": "Dependency Track processing timeout after maximum attempts",
                            "failed_at": timezone.now().isoformat(),
                        }
                    else:
                        # Schedule next attempt
                        logger.info(
                            f"[TASK_poll_dt_results] DT still processing SBOM {sbom_id}, "
                            f"scheduling attempt {current_attempt + 1}"
                        )
                        poll_dependency_track_results_task.send(
                            sbom_id, mapping_id, max_attempts, initial_delay, current_attempt + 1
                        )

                        return {
                            "status": "polling",
                            "sbom_id": sbom_id,
                            "attempt": current_attempt,
                            "next_attempt_in_seconds": delay * 2,
                            "message": f"DT still processing, will check again in {delay * 2} seconds",
                        }

            except Exception as api_error:
                logger.warning(f"[TASK_poll_dt_results] API error for SBOM {sbom_id}: {api_error}")

                if current_attempt >= max_attempts:
                    # Max attempts reached with API errors
                    error_results = {
                        "vulnerability_count": {"critical": 0, "high": 0, "medium": 0, "low": 0, "info": 0, "total": 0},
                        "findings": [],
                        "error": f"DT API error: {str(api_error)}",
                        "scan_metadata": {"api_error": True, "attempts": max_attempts},
                    }
                    service._cache_dt_results(sbom, mapping, error_results)

                    return {
                        "status": "error",
                        "sbom_id": sbom_id,
                        "attempt": current_attempt,
                        "error": str(api_error),
                        "failed_at": timezone.now().isoformat(),
                    }
                else:
                    # Retry on API error
                    poll_dependency_track_results_task.send(
                        sbom_id, mapping_id, max_attempts, initial_delay, current_attempt + 1
                    )

                    return {
                        "status": "retrying",
                        "sbom_id": sbom_id,
                        "attempt": current_attempt,
                        "error": str(api_error),
                        "message": "API error, will retry",
                    }

    except SBOM.DoesNotExist:
        logger.error(f"[TASK_poll_dt_results] SBOM {sbom_id} not found")
        return {
            "status": "error",
            "sbom_id": sbom_id,
            "error": "SBOM not found",
            "failed_at": timezone.now().isoformat(),
        }
    except Exception:
        logger.exception(f"[TASK_poll_dt_results] Unexpected error for SBOM {sbom_id}")
        return {
            "status": "error",
            "sbom_id": sbom_id,
            "error": "Processing error",
            "failed_at": timezone.now().isoformat(),
        }


@cron("0 */6 * * *")  # Run every 6 hours at minute 0
@dramatiq.actor(queue_name="dt_periodic_polling", max_retries=2, time_limit=1800000, store_results=True)
@retry(
    retry=retry_if_exception_type((OperationalError, DatabaseError)),
    wait=wait_exponential(multiplier=1, min=2, max=10),
    stop=stop_after_delay(60),
    before_sleep=before_sleep_log(logger, logging.WARNING),
)
def periodic_dependency_track_polling_task(hours_back: int = 6) -> Dict[str, Any]:
    """
    Periodic task to poll Dependency Track for vulnerability updates on existing projects - runs every 6 hours.

    This task runs every 6 hours to check for new vulnerabilities on existing DT projects
    without re-uploading SBOMs. DT continuously monitors for new CVEs, so we need to
    poll regularly to get updated vulnerability data.

    Args:
        hours_back: Only poll projects that haven't been checked in the last N hours

    Returns:
        Dictionary with polling statistics and results
    """
    from datetime import timedelta

    from django.db import models

    from ..models import ComponentDependencyTrackMapping
    from ..services import VulnerabilityScanningService

    logger.info(f"[TASK_dt_periodic_polling] Starting periodic DT polling (last {hours_back}h)")

    try:
        with transaction.atomic():
            connection.ensure_connection()

            # Find DT mappings that haven't been polled recently
            recent_threshold = timezone.now() - timedelta(hours=hours_back)

            # Get mappings that either:
            # 1. Haven't been synced recently, OR
            # 2. Have been uploaded but not polled in the last N hours
            stale_mappings = (
                ComponentDependencyTrackMapping.objects.filter(
                    dt_server__is_active=True, dt_server__health_status__in=["healthy", "degraded"]
                )
                .filter(models.Q(last_metrics_sync__lt=recent_threshold) | models.Q(last_metrics_sync__isnull=True))
                .select_related("component", "dt_server")
            )

            if not stale_mappings.exists():
                logger.info("[TASK_dt_periodic_polling] No stale DT mappings found")
                return {
                    "status": "completed",
                    "mappings_checked": 0,
                    "successful_polls": 0,
                    "failed_polls": 0,
                    "message": "No mappings needed polling",
                }

            results = {
                "status": "completed",
                "mappings_checked": stale_mappings.count(),
                "successful_polls": 0,
                "failed_polls": 0,
                "errors": [],
                "updated_projects": [],
            }

            for mapping in stale_mappings[:50]:  # Limit to 50 per run to avoid overload
                service = VulnerabilityScanningService()
                try:
                    # Get the latest SBOM for this component
                    latest_sbom = mapping.component.sbom_set.order_by("-created_at").first()

                    if not latest_sbom:
                        logger.warning(
                            f"[TASK_dt_periodic_polling] No SBOMs found for component {mapping.component.id}"
                        )
                        continue

                    logger.debug(
                        f"[TASK_dt_periodic_polling] Polling {mapping.component.name} project {mapping.dt_project_name}"
                    )

                    # Poll for updated vulnerability data
                    poll_results = service.get_dependency_track_results(latest_sbom, mapping, force_refresh=True)

                    # Store the updated results
                    service._cache_dt_results(latest_sbom, mapping, poll_results)

                    # Update the sync timestamp
                    mapping.last_metrics_sync = timezone.now()
                    mapping.save(update_fields=["last_metrics_sync"])

                    results["successful_polls"] += 1
                    results["updated_projects"].append(
                        {
                            "component": mapping.component.name,
                            "project_uuid": str(mapping.dt_project_uuid),
                            "vulnerabilities": poll_results.get("vulnerability_count", {}).get("total", 0),
                        }
                    )

                    logger.debug(f"[TASK_dt_periodic_polling] Successfully polled {mapping.component.name}")

                except Exception as poll_error:
                    logger.warning(f"[TASK_dt_periodic_polling] Failed to poll mapping {mapping.id}: {poll_error}")
                    results["failed_polls"] += 1
                    results["errors"].append({"component": mapping.component.name, "error": str(poll_error)})

            logger.info(
                f"[TASK_dt_periodic_polling] Completed periodic polling. "
                f"Checked {results['mappings_checked']} mappings, "
                f"{results['successful_polls']} successful, {results['failed_polls']} failed"
            )

            return results

    except Exception:
        logger.exception("[TASK_dt_periodic_polling] Periodic DT polling failed")
        return {"status": "failed", "error": "Task failed", "failed_at": timezone.now().isoformat()}


@cron("0 * * * *")  # Run every hour at minute 0
@dramatiq.actor(queue_name="dt_hourly_setup", max_retries=2, time_limit=900000, store_results=True)
@retry(
    retry=retry_if_exception_type((OperationalError, DatabaseError)),
    wait=wait_exponential(multiplier=1, min=2, max=15),
    stop=stop_after_delay(120),
    before_sleep=before_sleep_log(logger, logging.WARNING),
)
def recurring_dependency_track_backfill_task(
    team_key: str = None, max_components: int = 50, dry_run: bool = False
) -> Dict[str, Any]:
    """
    Hourly scheduled task to ensure components from workspaces using DT have proper mappings and uploaded SBOMs.

    Automatically runs every hour to quickly handle cases where:
    - Workspaces upgrade billing plans (Community → Business/Enterprise)
    - Workspaces switch vulnerability provider (OSV → Dependency Track)
    - Workspaces add custom DT servers (Enterprise feature)
    - Components exist that should use DT but have no ComponentDependencyTrackMapping

    These components are ignored by periodic polling since it only works on existing mappings.

    Strategy:
    1. Find components belonging to workspaces configured for DT
    2. Filter to components that have NO DT mappings yet
    3. Upload latest SBOM + SBOMs from tagged releases (same logic as weekly scan)
    4. Rate-limited to 50 components per hour to avoid overwhelming DT servers

    Args:
        team_key: Only process components for a specific workspace (for testing)
        max_components: Maximum number of components to process per run (default: 50)
        dry_run: If True, only identify components without uploading

    Returns:
        Dictionary with setup statistics and results
    """
    logger.info(f"[TASK_dt_hourly_setup] Starting hourly DT component check (dry_run={dry_run})")

    try:
        connection.ensure_connection()

        from ..services import DependencyTrackSetupService

        # Use the dedicated service for DT setup operations
        setup_service = DependencyTrackSetupService()

        # Find components that need DT setup
        components_needing_setup = setup_service.find_components_needing_setup(team_key, max_components)

        if not components_needing_setup:
            logger.info("[TASK_dt_hourly_setup] No components found that need DT setup")
            return {
                "status": "completed",
                "dry_run": dry_run,
                "components_identified": 0,
                "sboms_processed": 0,
                "successful_uploads": 0,
                "failed_uploads": 0,
                "message": "No components need DT setup",
            }

        if dry_run:
            # Just report what would be processed
            logger.info(f"[TASK_dt_hourly_setup] DRY RUN: Would process {len(components_needing_setup)} components")
            return {
                "status": "completed",
                "dry_run": True,
                "components_identified": len(components_needing_setup),
                "components": [
                    {
                        "id": str(comp.id),
                        "name": comp.name,
                        "team": comp.team.key,
                        "sbom_count": comp.sbom_set.count(),
                        "latest_sbom_date": comp.latest_sbom.created_at.isoformat() if comp.latest_sbom else None,
                    }
                    for comp in components_needing_setup
                ],
            }

        # Process components using the service
        results = setup_service.process_components_for_setup(components_needing_setup)

        logger.info(
            f"[TASK_dt_hourly_setup] Recurring check completed in {results['processing_time_ms']}ms. "
            f"Processed {results['components_processed']} components, "
            f"{results['successful_uploads']} successful uploads, "
            f"{results['failed_uploads']} failed uploads. "
            f"Performance: avg download {results['performance_metrics']['avg_sbom_download_time_ms']}ms, "
            f"avg upload {results['performance_metrics']['avg_upload_time_ms']}ms, "
            f"total data {results['performance_metrics']['total_data_processed_bytes']} bytes"
        )

        return {"status": "completed", "dry_run": False, **results}

    except Exception as e:
        logger.exception(f"[TASK_dt_hourly_setup] Recurring DT component check failed: {e}")
        return {"status": "failed", "error": f"Task failed: {str(e)}", "failed_at": timezone.now().isoformat()}


# Helper functions have been moved to DependencyTrackSetupService in services.py
# to improve code organization and maintainability
